# Configuration for Rasa NLU.
# https://rasa.com/docs/rasa/nlu/components/
language: en

pipeline:
# # No configuration for the NLU pipeline was provided. The following default pipeline was used to train your model.
# # If you'd like to customize it, uncomment and adjust the pipeline.
# # See https://rasa.com/docs/rasa/tuning-your-model for more information.
#   - name: WhitespaceTokenizer
#   - name: RegexFeaturizer
#   - name: LexicalSyntacticFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   - name: DIETClassifier
#     epochs: 100
#   - name: EntitySynonymMapper
#   - name: ResponseSelector
#     epochs: 100
#   - name: FallbackClassifier
#     threshold: 0.3
#     ambiguity_threshold: 0.1

- name: "SpacyNLP"
  model: "en_core_web_lg"
  case_sensitive: false
- name: SpacyTokenizer
# Lexical syntactic featurizer is additional as well.
# - name: LexicalSyntacticFeaturizer
#   features:
#     - [low, title, upper, suffix2] # features for the word preceding the word being evaluated
#     - [EOS, title] # features for the word being evaluated
#     - [prefix2]
- name: SpacyFeaturizer
- name: CRFEntityExtractor
- name: DIETClassifier
  epochs: 100 # Default was 300, set to 100 to prevent overfitting.
  entity_recognition: False
  # These are additional features.
  use_masked_language_model: True
  # embedding_dimension: 30
  # transformers_layers: 4
  # Increasing embedding dimension from the default 20 can give better results. embedding_dimension: 30
  # For example, in "Play a game", "Watch a game", and "Watch a play", game and play have slightly different meanings. Knowing the nuances will help identify the correct intent.
  # If at some point you encounter confusions in such intents, you may try to increase the number of transformers_layers. The default value is 2. transformer_layers: 4


# Configuration for Rasa Core.
# https://rasa.com/docs/rasa/core/policies/
policies:
# # No configuration for policies was provided. The following default policies were used to train your model.
# # If you'd like to customize them, uncomment and adjust the policies.
# # See https://rasa.com/docs/rasa/policies for more information.
#   - name: MemoizationPolicy
#   - name: TEDPolicy
#     max_history: 5
#     epochs: 100
#   - name: RulePolicy



# Good for typing errors.When using n-grams, you need to consider training time. Adding features by a factor of 24 will have an impact on your training. Choose a combination of n-grams that brings enough resilience to typing mistakes.
# pipeline:
#   - name: WhitespaceTokenizer
#   - name: CountVectorsFeaturizer
#     analyzer: word
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 2
#     max_ngram: 4
#   - name: DIETClassifier

# Can use HFTransformersNLP as well. Here, we can use GPT2. Try it out.
# pipeline:
#   - name: HFTransformersNLP
#     # Name of the language model to use
#     model_name: "bert"
#     # Pre-Trained weights to be loaded
#     model_weights: "bert-base-uncased"
#     cache_dir: /app/models/.cache # required with Botfront
#   - name: LanguageModelTokenizer
#   - name: LanguageModelFeaturizer
#   - name: CountVectorsFeaturizer
#   - name: CountVectorsFeaturizer
#     analyzer: char_wb
#     min_ngram: 1
#     max_ngram: 4
#   - name: DIETClassifier


# We can use lexical syntactic featurizer as well.